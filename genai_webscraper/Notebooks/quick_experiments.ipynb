{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asynchronous function execution \n",
    "import pickle \n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import time\n",
    "\n",
    "from pprint import pprint \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at 17:41:14\n",
      "hello\n",
      "finished at 17:41:15\n",
      "world\n",
      "finished at 17:41:16\n"
     ]
    }
   ],
   "source": [
    "async def say_after(delay, what):\n",
    "    await asyncio.sleep(delay)\n",
    "    print(what)\n",
    "\n",
    "async def main():\n",
    "    # both tasks are ran at the same time however the second task takes longer to complete without blocking the first task \n",
    "    task1 = asyncio.create_task(\n",
    "        say_after(1, 'hello'))\n",
    "\n",
    "    task2 = asyncio.create_task(\n",
    "        say_after(2, 'world'))\n",
    "\n",
    "    print(f\"started at {time.strftime('%X')}\")\n",
    "    await task1\n",
    "    print(f\"finished at {time.strftime('%X')}\")\n",
    "    await task2\n",
    "    print(f\"finished at {time.strftime('%X')}\")\n",
    "\n",
    "# Use await directly\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...task 1\n",
      "Fetching more data...task 2\n",
      "task 1 Data received!\n",
      "task 2 data received!\n"
     ]
    }
   ],
   "source": [
    "# this is nearly the same example as above\n",
    "# however here we have a function for each unique task \n",
    "# where as above the same function was used to create both tasks \n",
    "\n",
    "nest_asyncio.apply() # allows nested use of asyncio.run inside notebooks\n",
    "\n",
    "async def fetch_data():\n",
    "    print(\"Fetching data...task 1\")\n",
    "    await asyncio.sleep(1)  # Simulates a non-blocking I/O operation\n",
    "    return \"task 1 Data received!\"\n",
    "\n",
    "async def fetch_more_data():\n",
    "    print(\"Fetching more data...task 2\")\n",
    "    await asyncio.sleep(3)  # Simulates a different non-blocking I/O operation\n",
    "    return \"task 2 data received!\"\n",
    "\n",
    "async def main():\n",
    "    # Start both co-routines concurrently\n",
    "    task1 = asyncio.create_task(fetch_data())\n",
    "    task2 = asyncio.create_task(fetch_more_data())\n",
    "\n",
    "    #### Case 1) \n",
    "    # Wait for both tasks to complete before main() terminates \n",
    "    # await python keyword indicates that both tasks will be ran asynchronously \n",
    "    # await keyword is used inside functions that are declared with with \"async def\"\n",
    "    \n",
    "    # The use of \"async\" and \"await\" means that the function will wait for each task to finish running\n",
    "    # without blocking the execution of other tasks \n",
    "    result1 = await task1\n",
    "    result2 = await task2\n",
    "    \n",
    "    #### Case 2)\n",
    "    # Don't wait for task 2 to complete before main() terminates \n",
    "    # Result: Function terminates without \"task 2 data received!\" print out\n",
    "    # Function waited for task 1 to terminate only \n",
    "    # result1 = await task1\n",
    "    # result2 =  task2\n",
    "    \n",
    "    #### Case 2)\n",
    "    # Don't wait for task 1 nor task 2 to complete before main() terminates\n",
    "    # Result: Function terminated without waiting for either task to complete \n",
    "    # As soon as the function ran out of lines of code to run, it terminated \n",
    "    # Neither \"task 1 Data received!\" nor \"task 2 data received!\" printed out \n",
    "    # result1 = task1\n",
    "    # result2 = task2\n",
    "\n",
    "    print(result1)\n",
    "    print(result2)\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply() # allows nested use of asyncio.run within jupyter notebooks\n",
    "\n",
    "\n",
    "async def scrape_tweets(): # main function \n",
    "    \n",
    "    async with async_playwright() as p: \n",
    "        \n",
    "        # Launch the browser and open a new page\n",
    "        browser = await p.chromium.launch(headless=True)  # Set headless=True if you want to run it in the background\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        # Maximize the window\n",
    "        await page.set_viewport_size({\"width\": 1920, \"height\": 1080})\n",
    "        await page.goto(\"https://x.com/apify\")\n",
    "        \n",
    "        # Wait for the page to load completely\n",
    "        await page.wait_for_timeout(10000)  # Wait for 10 seconds\n",
    "\n",
    "        tweets = set()  # Use a set to avoid duplicates\n",
    "\n",
    "        while len(tweets) < 5:  # Continue scrolling until we have at least 20 tweets\n",
    "            # Locate all the div elements with attribute name \"lang\"\n",
    "            tweet = \"\"\n",
    "            elements = await page.locator('div[lang]').element_handles()\n",
    "            for element in elements:\n",
    "                text = await element.inner_text()\n",
    "                tweets.add(text)\n",
    "\n",
    "            # Scroll down\n",
    "            await page.evaluate('window.scrollBy(0, window.innerHeight)')\n",
    "            await page.wait_for_timeout(2000)  # Wait for 2 seconds to load more tweets\n",
    "            \n",
    "            \n",
    "        \n",
    "        await browser.close()\n",
    "        \n",
    "    # save tweets\n",
    "    with open('my_list.pkl', 'wb') as f:\n",
    "        pickle.dump(tweets, f) \n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(scrape_tweets())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_list.pkl', 'rb') as f:  # Use 'rb' to read the file in binary mode\n",
    "    my_list = list(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"And finally there's also an universal #webscraping and #crawling library for #JavaScript / #NodeJS, similar to \\n@ScrapyProject\\n for #Python that was around for years, but also working with headless Chrome and Puppeteer. Better late than never  https://github.com/apifytech/apify-js‚Ä¶\",\n",
       " '15,000+ Stars on \\n@GitHub\\n  \\n\\nWe are giving away 5 Crawlee hoodies to celebrate this milestone \\n\\nRules:\\n\\n- Follow \\n@apify\\n \\n- Retweet this tweet \\n- Star us at: https://apify.it/3TDOig8 \\n- Reply with your GitHub username \\n\\n Deadline: October 1st 2024',\n",
       " 'Today we‚Äôre launching Crawlee on Product Hunt \\n \\nPlease show us some  and support, to encourage the team who spent years building this open-source library!\\n \\nLearn more about Crawlee in the thread \\n \\n1/5\\n\\nhttps://producthunt.com/posts/crawlee',\n",
       " '12,500+ Stars on \\n@GitHub\\n \\n\\nWe are giving away 3 Crawlee shirts to celebrate this milestone \\n\\nRules:\\n\\n- Follow \\n@apify\\n \\n- Retweet this tweet \\n- Star us at: https://apify.it/3UZ22lv \\n- Reply with your GitHub username \\n\\n Deadline: 7th June 2024',\n",
       " 'That moment when \\n@Microsoft\\n starts using your open-source web scraping libraries...  https://github.com/microsoft/accessibility-insights-service‚Ä¶ #webscraping #opensource',\n",
       " 'We are now 7.5K dev friends on our Discord channel, which means a swag drop: 3 exclusive Apify water bottles! \\n\\n- RT this tweet \\n- Tag a dev friend \\n- Star Crawlee: https://apify.it/3VDQcxG\\n\\n Deadline: July 1\\n\\nMore such giveaways on our Discord: https://apify.it/3W8NMIV']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def custom_extractor(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "loader = RecursiveUrlLoader(\n",
    "    \"https://glendalehs.gusd.net/\",\n",
    "    max_depth=2,\n",
    "    use_async=True,\n",
    "    extractor=None,\n",
    "    metadata_extractor=None,\n",
    "    exclude_dirs=(),\n",
    "    timeout=10,\n",
    "    check_response_status=True,\n",
    "    continue_on_failure=True,\n",
    "    prevent_outside=True,\n",
    "    base_url=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to load https://glendalehs.gusd.net/8182423161. Received error Received HTTP status 404 of type ValueError\n",
      "Unable to load https://glendalehs.gusd.net/8182446309. Received error Received HTTP status 404 of type ValueError\n",
      "Unable to load https://glendalehs.gusd.net/29730_2. Received error Cannot connect to host www.ghsclassof73reunion.com:443 ssl:default [nodename nor servname provided, or not known] of type ClientConnectorDNSError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    <!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <!-- needed for form submission -->\n",
      "<script id=\"goo\n",
      "{'source': 'https://glendalehs.gusd.net/', 'content_type': 'text/html; charset=UTF-8', 'title': 'Glendale High School - Home', 'description': 'Glendale High School', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "docs_lazy = loader.lazy_load()\n",
    "\n",
    "# async variant:\n",
    "# docs_lazy = await loader.alazy_load()\n",
    "\n",
    "for doc in docs_lazy:\n",
    "    docs.append(doc)\n",
    "print(docs[0].page_content[:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object async_generator can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 31\u001b[0m\n\u001b[1;32m     15\u001b[0m loader \u001b[38;5;241m=\u001b[39m RecursiveUrlLoader(url\u001b[38;5;241m=\u001b[39murl, \n\u001b[1;32m     16\u001b[0m                             extractor\u001b[38;5;241m=\u001b[39mcustom_extractor, \n\u001b[1;32m     17\u001b[0m                             max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;66;03m# TODO: incorporate chunking so we can scrap more than a single page \u001b[39;00m\n\u001b[1;32m     18\u001b[0m                             use_async\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load the data from the website ( 3 options )\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m##  1) This method is blocking and synchronous. It fully loads all documents into memory in one go before the function returns.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m## 3) This method is used in an asynchronous context. It allows for non-blocking, asynchronous fetching of documents. \u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m##    You can perform other tasks while waiting for documents to be fetched.\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loader\u001b[38;5;241m.\u001b[39malazy_load()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Define the file path to store the data\u001b[39;00m\n\u001b[1;32m     34\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscraped__data.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: object async_generator can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "# scraps tab names and drop down menu titles \n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# Define the URL of your college website\n",
    "url = \"https://glendalehs.gusd.net/\"\n",
    "\n",
    "# Define a custom extractor function to extract text from HTML using BeautifulSoup\n",
    "def custom_extractor(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Instantiate the RecursiveUrlLoader\n",
    "# TODO: Sanity Check performance of RecursiveUrlLoader vs. Playwrite for crawling capabilities \n",
    "loader = RecursiveUrlLoader(url=url, \n",
    "                            extractor=custom_extractor, \n",
    "                            max_depth=1, # TODO: incorporate chunking so we can scrap more than a single page \n",
    "                            use_async=True)\n",
    "\n",
    "# Load the data from the website ( 3 options )\n",
    "\n",
    "##  1) This method is blocking and synchronous. It fully loads all documents into memory in one go before the function returns.\n",
    "# docs = loader.load()  \n",
    "\n",
    "## 2) This method returns a generator. Instead of loading all documents at once, it yields documents one by one as they are fetched. \n",
    "##    This means you can process each document as soon as it is available, without waiting for all documents to load.\n",
    "#docs = loader.lazy_load()\n",
    "\n",
    "## 3) This method is used in an asynchronous context. It allows for non-blocking, asynchronous fetching of documents. \n",
    "##    You can perform other tasks while waiting for documents to be fetched.\n",
    "docs = await loader.alazy_load()\n",
    "\n",
    "# Define the file path to store the data\n",
    "output_file = \"scraped__data.txt\"\n",
    "\n",
    "# Open the file in write mode with UTF-8 encoding\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    # Write metadata and content for each document to the file\n",
    "    for doc in docs:\n",
    "        title = doc.metadata.get(\"title\")\n",
    "        source = doc.metadata.get(\"source\")\n",
    "        content = doc.page_content\n",
    "\n",
    "        # Ensure that the title, source, and content are string type\n",
    "        if isinstance(title, str) and isinstance(source, str) and isinstance(content, str):\n",
    "            file.write(\"Page Title: \" + title + \"\\n\")\n",
    "            file.write(\"Page URL: \" + source + \"\\n\")\n",
    "            file.write(\"Page Content:\\n\" + content + \"\\n\\n\")\n",
    "        else:\n",
    "            print(\"Skipped a document due to non-string content.\")\n",
    "\n",
    "print(\"Data has been successfully written to\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Crawler\n",
    "\n",
    "#### Version 1 of Crawler works \n",
    "- Scraps multiple pages \n",
    "- Filters text for activities and events \n",
    "- Crafts engaging email for students and includes a link \n",
    "\n",
    "#### ToDo\n",
    "- Move text to VectorDB\n",
    "- Scrap Images of events \n",
    "- Scrap Names and Titles of Administrators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from playwright.async_api import async_playwright, Playwright\n",
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_chroma import Chroma\n",
    "from openai import OpenAI\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# apply nest_asyncio to enable running async functions in Jupyter\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### playwright TAKES TOO LONG TO SCRAPE, RETURNS REDUNDANT RESULTS, AND SCRAPS TOO MUCH SUB-PAGES\n",
    "### TODO: maybe stop using async_playwright to stop redundant results \n",
    "### TODO: FILTER NUMBER OF SUB-PAGES IT SCRAPS \n",
    "class Crawler():\n",
    "    def __init__(self):\n",
    "        self.async_playwright =  async_playwright\n",
    "    \n",
    "    def custom_extractor(self, html_content):\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "    # Step 3: Define an async function to load documents using loader.lazy_load()\n",
    "    async def fetch_documents(self, url):\n",
    "        # Initialize the RecursiveUrlLoader with a sample URL\n",
    "        # TODO: REPLACE RecursiveUrlLoader WITH PLAYWRITE FOR DYNAMICALLY LOADED WEBSITES \n",
    "        loader = RecursiveUrlLoader(url = url, \n",
    "                                    extractor=self.custom_extractor, \n",
    "                                    max_depth=1, # TODO: incorporate chunking so we can scrap more than a single page \n",
    "                                    use_async=True)\n",
    "\n",
    "            \n",
    "        return [doc for doc in  loader.lazy_load()]\n",
    "    \n",
    "    async def crawl(self, url):\n",
    "        docs =  await self.fetch_documents(url) \n",
    "        return docs\n",
    "    \n",
    "    async def fetch_documents_dynamic(self, url, playwright: Playwright):\n",
    "        chromium = playwright.chromium # or \"firefox\" or \"webkit\".\n",
    "        browser = await chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        texts = await page.get_by_role(\"link\").all_text_contents()\n",
    "        await browser.close()\n",
    "        return texts\n",
    "\n",
    "    async def crawl_dynamic(self, url):\n",
    "        async with self.async_playwright() as playwright:\n",
    "            docs = await self.fetch_documents_dynamic(url, playwright)\n",
    "            return set(docs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterWebPage():\n",
    "    def __init__(self, directory=None):\n",
    "        self.llm = OpenAI()\n",
    "        self.clean_doc = \"\"\n",
    "        \n",
    "        if directory is None: \n",
    "            self.dir = \"../scraped_sites\"\n",
    "        else:\n",
    "            self.dir = directory\n",
    "        \n",
    "    def filename_prompt(self, URL):\n",
    "        return f\"\"\"\n",
    "            Use this URL to create a simple and descriptive file name for a text file in python.  \n",
    "            The URL will contain the name of a school, focus on extracting the name of the school. \n",
    "            URL {URL} \n",
    "            Only return the file name.\n",
    "            \"\"\"\n",
    "\n",
    "    def crawler_prompt(self, doc):\n",
    "        return f\"\"\"\n",
    "            The following document is the results of scrapping a website. Many unless words have been scrapped such as the title of drop down tabs and their sub-titles. \n",
    "            For example, \"Athletics\" is the title of a drop down tab and \"Sports Calendar\" and \"Coach Contact Information\" is unhelpful information. \n",
    "            Conversely, there is useful information in the document that describes specific events and activities that have taken place or will take place. \n",
    "            This useful information is usually, but not always, provided as a phrase, sentence, or paragraph, make sure they have sufficient context \n",
    "            in order to understand what kind of event or activity it is, an event or activity without no context isn't useful. For example \"Garden Club\" on its own is not useful. \n",
    "\n",
    "            Your goal is to identify and maintain the useful information while removing the useless information. \n",
    "            Attempt to identify related information and output it as a single string. \n",
    "            For example, if there is a dance event with dates and contact information, that should all be a single string \n",
    "            where as information about a completely different event or activity (such as a fundraiser) should be a separate string that you output. \n",
    "            Separate the strings using a newline character. \n",
    "\n",
    "            If a document doesn't have any useful information, return a white space.\n",
    "\n",
    "            Document: {doc}\n",
    "            \"\"\"\n",
    "        \n",
    "    def get_filename(self, URL):\n",
    "        completion =  OpenAI().chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        max_tokens=5500,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", f\"content\": self.filename_prompt(URL)}\n",
    "            ]\n",
    "        )\n",
    "        self.filename = completion.choices[0].message.content\n",
    "        self.school_name = completion.choices[0].message.content.strip(\".txt\")\n",
    "\n",
    "    def filter_doc(self, doc):\n",
    "        for doc in docs: \n",
    "            completion = self.llm.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                max_tokens=5500,\n",
    "                temperature=0.0,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.crawler_prompt(doc)}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            self.clean_doc = completion.choices[0].message.content\n",
    "\n",
    "        ## TODO: NEED TO AGGRAGATE RESULTS \n",
    "        \n",
    "    def save_clean_doc(self):\n",
    "        fn = \"/\".join((self.dir, self.filename))\n",
    "        #doc = \" \".join(self.clean_doc)\n",
    "        with open(fn, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(self.clean_doc)\n",
    "            \n",
    "\n",
    "class VectorStore():\n",
    "    def __init__(self):\n",
    "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "        self.vector_store = Chroma(\n",
    "                collection_name=\"school_collection\",\n",
    "                embedding_function=self.embeddings,\n",
    "                persist_directory=\"../chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    "            )\n",
    "\n",
    "    def add_document(self, doc, metadata):\n",
    "        # TODO\n",
    "        # need to use hashtags or metadata for actual IDing \n",
    "        # then use the query to further filter the type of docs from that school \n",
    "        # alternative: create human readable ids like \"<school_name>_<doc_type>_<datetime>_<doc number>\"\n",
    "        document = Document(\n",
    "            page_content=doc,\n",
    "            metadata={\"source\": metadata}\n",
    "        )\n",
    "        \n",
    "        documents = [document]\n",
    "        ids = [str(uuid4()) for _ in documents]\n",
    "        \n",
    "        # Add documents with UUIDs as IDs\n",
    "        self.vector_store.add_documents(documents=documents, ids=ids)\n",
    "    \n",
    "    def retrieve_document(self, query, metadata=None, k=1):\n",
    "        results = self.vector_store.similarity_search_with_relevance_scores(\n",
    "            query, # for now, ask for events/activities that take place at the school \n",
    "            k=k, # number of docs to retrieve\n",
    "            filter={\"source\": metadata}, # in the future we will also have documents with names and job titles\n",
    "        )\n",
    "        return results\n",
    "    \n",
    "    def get_school_names(self):\n",
    "        # all metadata tags are school names \n",
    "        # returns list of unique school names\n",
    "        return list(set(dic['source']  for dic in self.vector_store.get()['metadatas']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class PhishingEmail():\n",
    "    def __init__(self, phishing_link='https://example.org/'):\n",
    "        pass\n",
    "        self.today = datetime.today().date()\n",
    "        self.llm = OpenAI()\n",
    "        self.phishing_link = phishing_link\n",
    "        \n",
    "\n",
    "    def prompt(self, doc, phishing_link):\n",
    "        if phishing_link is not None:\n",
    "            self.phishing_link = phishing_link\n",
    "            \n",
    "\n",
    "        return f\"\"\"\n",
    "        The following document contains information about multiple school activities and events.  \n",
    "        Your task is to select one event and use that info to write a persuasive email to encourage a student to click on an event related link.\n",
    "        Keep the email short, simple, but most of all persuasive and use language that would appeal to a high school student such as use of emojis.\n",
    "        Keep today's date in mind as you describe event dates in the email, don't provide a past date for a future event.  \n",
    "\n",
    "        DOCUMENT: {doc}\n",
    "        LINK: {self.phishing_link}\n",
    "        TODAY'S DATE: {self.today}\n",
    "        \"\"\"\n",
    "\n",
    "    def create_email(self, doc, phishing_link=None):\n",
    "        completion = self.llm.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            max_tokens=5500,\n",
    "            temperature=0.0,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.prompt(doc, phishing_link)}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://rdwhite.gusd.net/', \n",
    "        'https://wilson.gusd.net/', \n",
    "        \"https://glendalehs.gusd.net/\"]\n",
    "\n",
    "filter_page = FilterWebPage()\n",
    "crawler = Crawler()\n",
    "vs = VectorStore()\n",
    "pe = PhishingEmail()\n",
    "\n",
    "for url in urls: \n",
    "        ### scrape URL\n",
    "        docs = await crawler.crawl(url) \n",
    "        #docs = asyncio.run(crawler.crawl_dynamic(url))\n",
    "        # filter scaped pages for useful content \n",
    "        filter_page.filter_doc(docs) \n",
    "        # create filename using school name from URL\n",
    "        filter_page.get_filename(url)\n",
    "        # save to file \n",
    "        # filter_page.save_clean_doc()\n",
    "        #TODO: this workflow assumes that only a single page regarding activities is in the results\n",
    "        # not true once we start scraping names and titles \n",
    "        vs.add_document(filter_page.clean_doc, filter_page.school_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loop's logic works because there is currently one doc per school \n",
    "school_names = vs.get_school_names()\n",
    "\n",
    "\n",
    "emails = []\n",
    "for school in school_names:\n",
    "    doc = vs.retrieve_document(\"activities and events for glendale high school\", school)\n",
    "    doc = doc[0][0].page_content\n",
    "    phishing_email = pe.create_email(doc)\n",
    "    emails.append(phishing_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject: üåü Exciting Opportunity: Armenian Dual Immersion Program! üåü\\n\\nHey there!\\n\\nAre you ready to take your language skills to the next level? üó£Ô∏è‚ú® R.D. White Elementary School is now accepting applications for the Armenian Dual Immersion Program for the 2025-26 school year! This is an amazing chance to immerse yourself in the Armenian language and culture while making new friends and having fun! üéâ\\n\\nImagine being part of a vibrant community where you can learn and grow together. Plus, it looks great on college applications! üìöüí™\\n\\nDon‚Äôt miss out! Click the link below to learn more and apply today! ‚¨áÔ∏è\\n\\nüëâ [Apply Now!](https://example.org/)\\n\\nLet‚Äôs make your high school experience unforgettable! \\n\\nBest,  \\n[Your Name]',\n",
       " \"Subject: üéì Don't Miss Out on Financial Aid Workshop! üí∞\\n\\nHey [Student's Name]! \\n\\nAre you ready to tackle college expenses? üè´üí∏ Join us for the **Understanding Financial Aid for College** workshop on **Wednesday, Dec. 4th from 6:30 - 8:30 pm** at Hoover High School! This is your chance to get all the info you need about financial aid options, scholarships, and how to make college more affordable. \\n\\nDon‚Äôt let money worries hold you back from your dreams! üåü Click the link below to register and secure your spot:\\n\\nüëâ [Register Here!](https://example.org/)\\n\\nLet‚Äôs make your college journey a little easier! See you there! \\n\\nBest,  \\n[Your Name]  \\n\",\n",
       " \"Subject: üéâ Don't Miss Out on the Fall Dance! üï∫üíÉ\\n\\nHey [Student's Name]! \\n\\nI hope you're having an awesome day! üåü I wanted to remind you about the Fall Dance happening on **November 17th** at Woodrow Wilson Middle School! üé∂‚ú® It‚Äôs going to be a night full of fun, friends, and fantastic music! \\n\\nImagine dancing the night away with your friends, enjoying delicious snacks, and making unforgettable memories! üéâ Plus, it‚Äôs a great chance to show off your best dance moves! üíÉüï∫\\n\\nYou definitely don‚Äôt want to miss this! Click the link below for more details and to get your tickets! üéüÔ∏èüëá  \\n[Get Your Tickets Here!](https://example.org/)\\n\\nLet‚Äôs make this dance the best one yet! Can‚Äôt wait to see you there! \\n\\nBest,  \\n[Your Name]  \\nWoodrow Wilson Middle School üåü\"]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Subject: üéâ Don't Miss Out on the Fall Dance! üï∫üíÉ\\n\"\n",
      " '\\n'\n",
      " \"Hey [Student's Name]! \\n\"\n",
      " '\\n'\n",
      " \"I hope you're having an awesome day! üåü I wanted to remind you about the Fall \"\n",
      " 'Dance happening on **November 17th** at Woodrow Wilson Middle School! üé∂‚ú® '\n",
      " 'It‚Äôs going to be a night full of fun, friends, and fantastic music! \\n'\n",
      " '\\n'\n",
      " 'Imagine dancing the night away with your friends, enjoying delicious snacks, '\n",
      " 'and making unforgettable memories! üéâ Plus, it‚Äôs a great chance to show off '\n",
      " 'your best dance moves! üíÉüï∫\\n'\n",
      " '\\n'\n",
      " 'You definitely don‚Äôt want to miss this! Click the link below for more '\n",
      " 'details and to get your tickets! üéüÔ∏èüëá  \\n'\n",
      " '[Get Your Tickets Here!](https://example.org/)\\n'\n",
      " '\\n'\n",
      " 'Let‚Äôs make this dance the best one yet! Can‚Äôt wait to see you there! \\n'\n",
      " '\\n'\n",
      " 'Best,  \\n'\n",
      " '[Your Name]  \\n'\n",
      " 'Woodrow Wilson Middle School üåü')\n"
     ]
    }
   ],
   "source": [
    "pprint(emails[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_webscraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
